#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åœ°çƒç§‘å­¦RSSç›‘æ§ç³»ç»Ÿ - å¢å¼ºç‰ˆ
Enhanced Geoscience RSS Monitoring System with Zone Scoring

Features:
- RSSSourceFinder for automatic RSS source discovery
- Zone-based article scoring system for quality prioritization
- Dynamic phrase extraction for trend analysis
- Improved error handling and retry mechanisms
- Enhanced push content formatting with translation support
- Detailed statistics collection

Author: Enhanced by AI Assistant
Date: 2025
"""

import os
import sys
import logging
import json
import csv
import re
import time
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter, defaultdict
from dataclasses import dataclass
from pathlib import Path

import feedparser
import requests
import jieba
import numpy as np
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from googletrans import Translator
import pandas as pd
import pytz
from tenacity import retry, stop_after_attempt, wait_exponential
import yaml

# é…ç½®è·¯å¾„
CONFIG_DIR = Path(__file__).parent.parent / "config"
DATA_DIR = Path(__file__).parent.parent / "data"
LOG_DIR = Path(__file__).parent.parent / "logs"

# ç¡®ä¿ç›®å½•å­˜åœ¨
LOG_DIR.mkdir(exist_ok=True)
DATA_DIR.mkdir(exist_ok=True)

# é…ç½®logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_DIR / 'sniffer.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class Article:
    """æ–‡ç« æ•°æ®ç»“æ„"""
    title: str
    link: str
    summary: str
    published: datetime
    source: str
    journal_zone: int = 4
    score: float = 0.0
    matched_keywords: List[str] = None
    
    def __post_init__(self):
        if self.matched_keywords is None:
            self.matched_keywords = []

@dataclass 
class JournalInfo:
    """æœŸåˆŠä¿¡æ¯æ•°æ®ç»“æ„"""
    name: str
    impact_factor: float
    zone: int
    rss_url: str

class RSSSourceFinder:
    """RSSæºè‡ªåŠ¨å‘ç°å™¨"""
    
    def __init__(self):
        self.user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': self.user_agent})
        
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def discover_rss_feeds(self, base_url: str) -> List[str]:
        """ä»ç½‘ç«™å‘ç°RSSæº"""
        try:
            response = self.session.get(base_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            rss_feeds = []
            
            # æŸ¥æ‰¾RSSé“¾æ¥
            for link in soup.find_all('link', type='application/rss+xml'):
                href = link.get('href')
                if href:
                    if href.startswith('/'):
                        href = base_url.rstrip('/') + href
                    rss_feeds.append(href)
                    
            # æŸ¥æ‰¾å¸¸è§RSSæ¨¡å¼
            common_patterns = ['/rss', '/feed', '/feeds', '.rss', '.xml']
            for pattern in common_patterns:
                test_url = base_url.rstrip('/') + pattern
                if self._test_rss_url(test_url):
                    rss_feeds.append(test_url)
                    
            return list(set(rss_feeds))
            
        except Exception as e:
            logger.warning(f"æ— æ³•ä» {base_url} å‘ç°RSSæº: {e}")
            return []
    
    def _test_rss_url(self, url: str) -> bool:
        """æµ‹è¯•URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„RSSæº"""
        try:
            response = self.session.get(url, timeout=5)
            if response.status_code == 200:
                # ç®€å•æ£€æŸ¥æ˜¯å¦åŒ…å«RSSå†…å®¹
                content = response.text.lower()
                return 'rss' in content or 'feed' in content or '<item>' in content
        except:
            pass
        return False

class DynamicPhraseExtractor:
    """åŠ¨æ€çŸ­è¯­æå–å™¨"""
    
    def __init__(self, min_length=2, max_length=4):
        self.min_length = min_length
        self.max_length = max_length
        self.vectorizer = TfidfVectorizer(
            ngram_range=(min_length, max_length),
            max_features=1000,
            stop_words='english'
        )
        
    def extract_phrases(self, texts: List[str], top_k=20) -> List[Tuple[str, float]]:
        """ä»æ–‡æœ¬ä¸­æå–çƒ­é—¨çŸ­è¯­"""
        try:
            if not texts:
                return []
                
            # é¢„å¤„ç†æ–‡æœ¬
            processed_texts = []
            for text in texts:
                # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
                clean_text = re.sub(r'[^\w\s]', ' ', text.lower())
                processed_texts.append(clean_text)
            
            # TF-IDFå‘é‡åŒ–
            tfidf_matrix = self.vectorizer.fit_transform(processed_texts)
            feature_names = self.vectorizer.get_feature_names_out()
            
            # è®¡ç®—å¹³å‡TF-IDFåˆ†æ•°
            mean_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()
            
            # è·å–top kçŸ­è¯­
            top_indices = mean_scores.argsort()[-top_k:][::-1]
            phrases = [(feature_names[i], mean_scores[i]) for i in top_indices if mean_scores[i] > 0]
            
            return phrases
            
        except Exception as e:
            logger.error(f"çŸ­è¯­æå–å¤±è´¥: {e}")
            return []

class TranslationService:
    """ç¿»è¯‘æœåŠ¡"""
    
    def __init__(self):
        self.translator = Translator()
        self.cache = {}
        
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=8))
    def translate_text(self, text: str, target_lang='zh-cn') -> str:
        """ç¿»è¯‘æ–‡æœ¬"""
        if not text or len(text.strip()) == 0:
            return text
            
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{text[:100]}_{target_lang}"
        if cache_key in self.cache:
            return self.cache[cache_key]
            
        try:
            result = self.translator.translate(text, dest=target_lang)
            translated = result.text
            self.cache[cache_key] = translated
            return translated
        except Exception as e:
            logger.warning(f"ç¿»è¯‘å¤±è´¥ '{text[:50]}...': {e}")
            return text

class StatisticsCollector:
    """ç»Ÿè®¡æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self):
        self.stats_file = DATA_DIR / "statistics.json"
        self.daily_stats = defaultdict(int)
        self.load_stats()
        
    def load_stats(self):
        """åŠ è½½å†å²ç»Ÿè®¡æ•°æ®"""
        try:
            if self.stats_file.exists():
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    self.historical_stats = json.load(f)
            else:
                self.historical_stats = {}
        except Exception as e:
            logger.error(f"åŠ è½½ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
            self.historical_stats = {}
    
    def record_event(self, event_type: str, value=1):
        """è®°å½•äº‹ä»¶"""
        self.daily_stats[event_type] += value
        
    def record_journal_access(self, journal_name: str):
        """è®°å½•æœŸåˆŠè®¿é—®"""
        self.record_event(f"journal_{journal_name}")
        
    def record_keyword_match(self, keyword: str):
        """è®°å½•å…³é”®è¯åŒ¹é…"""
        self.record_event(f"keyword_{keyword}")
        
    def save_daily_stats(self):
        """ä¿å­˜æ¯æ—¥ç»Ÿè®¡"""
        today = datetime.now().strftime('%Y-%m-%d')
        self.historical_stats[today] = dict(self.daily_stats)
        
        try:
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.historical_stats, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
    
    def get_summary(self) -> Dict:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        return {
            'daily_stats': dict(self.daily_stats),
            'total_days': len(self.historical_stats),
            'last_update': datetime.now().isoformat()
        }

class EnhancedGeoSniffer:
    """å¢å¼ºç‰ˆåœ°çƒç§‘å­¦RSSç›‘æ§å™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_config(config_path)
        self.journals_info = self._load_journals_info()
        self.rss_finder = RSSSourceFinder()
        self.phrase_extractor = DynamicPhraseExtractor()
        self.translator = TranslationService() if self.config.get('ENABLE_TRANSLATION', False) else None
        self.stats = StatisticsCollector()
        
        # è®¾ç½®jiebaåˆ†è¯
        jieba.setLogLevel(logging.WARNING)
        
        logger.info("å¢å¼ºç‰ˆåœ°çƒç§‘å­¦RSSç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: Optional[str]) -> Dict:
        """åŠ è½½é…ç½®"""
        if config_path is None:
            config_path = CONFIG_DIR / "config.py"
            
        if not os.path.exists(config_path):
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return self._get_default_config()
            
        try:
            # åŠ¨æ€å¯¼å…¥é…ç½®æ¨¡å—
            spec = {}
            with open(config_path, 'r', encoding='utf-8') as f:
                exec(f.read(), spec)
            
            # ç§»é™¤å†…ç½®å˜é‡
            config = {k: v for k, v in spec.items() if not k.startswith('__')}
            logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return config
            
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'WECHAT_WEBHOOK': '',
            'RSS_FEEDS': [],
            'KEYWORDS': ['geology', 'geoscience', 'earth science'],
            'ZONE_SCORING_ENABLED': True,
            'ZONE_WEIGHTS': {1: 10.0, 2: 7.0, 3: 4.0, 4: 2.0},
            'DYNAMIC_PHRASE_EXTRACTION': True,
            'ENABLE_TRANSLATION': False,
            'MAX_RETRIES': 3,
            'RETRY_DELAY': 2,
            'ENABLE_DETAILED_STATS': True
        }
    
    def _load_journals_info(self) -> Dict[str, JournalInfo]:
        """åŠ è½½æœŸåˆŠä¿¡æ¯"""
        journals_file = DATA_DIR / "journals_1-260.csv"
        journals_info = {}
        
        try:
            if journals_file.exists():
                df = pd.read_csv(journals_file)
                for _, row in df.iterrows():
                    journal_info = JournalInfo(
                        name=row['journal_name'],
                        impact_factor=float(row['impact_factor']),
                        zone=int(row['zone']),
                        rss_url=row['rss_url']
                    )
                    journals_info[journal_info.name] = journal_info
                logger.info(f"å·²åŠ è½½ {len(journals_info)} ä¸ªæœŸåˆŠä¿¡æ¯")
            else:
                logger.warning(f"æœŸåˆŠä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {journals_file}")
                
        except Exception as e:
            logger.error(f"åŠ è½½æœŸåˆŠä¿¡æ¯å¤±è´¥: {e}")
            
        return journals_info
    
    def get_rss_sources(self) -> List[Tuple[str, int]]:
        """è·å–RSSæºåˆ—è¡¨ï¼Œè¿”å›(url, zone)å…ƒç»„"""
        sources = []
        
        if self.config.get('ZONE_SCORING_ENABLED', True) and self.journals_info:
            # ä½¿ç”¨æœŸåˆŠä¿¡æ¯ä¸­çš„RSSæº
            for journal_info in self.journals_info.values():
                sources.append((journal_info.rss_url, journal_info.zone))
        else:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„RSSæº
            for url in self.config.get('RSS_FEEDS', []):
                sources.append((url, 4))  # é»˜è®¤ä¸ºzone 4
                
        logger.info(f"è·å–åˆ° {len(sources)} ä¸ªRSSæº")
        return sources
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def fetch_rss_feed(self, url: str) -> Optional[feedparser.FeedParserDict]:
        """è·å–RSS feed"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                logger.warning(f"RSS feedè§£æè­¦å‘Š {url}: {feed.bozo_exception}")
            
            return feed
            
        except Exception as e:
            logger.error(f"è·å–RSS feedå¤±è´¥ {url}: {e}")
            self.stats.record_event("rss_fetch_error")
            return None
    
    def extract_articles(self, feed: feedparser.FeedParserDict, source_info: Tuple[str, int]) -> List[Article]:
        """ä»RSS feedä¸­æå–æ–‡ç« """
        articles = []
        url, zone = source_info
        
        try:
            source_name = feed.feed.get('title', url)
            self.stats.record_journal_access(source_name)
            
            for entry in feed.entries:
                try:
                    # è§£æå‘å¸ƒæ—¶é—´
                    pub_date = self._parse_date(entry)
                    
                    # è¿‡æ»¤24å°æ—¶å†…çš„æ–‡ç« 
                    if self._is_recent_article(pub_date):
                        article = Article(
                            title=entry.get('title', ''),
                            link=entry.get('link', ''),
                            summary=entry.get('summary', ''),
                            published=pub_date,
                            source=source_name,
                            journal_zone=zone
                        )
                        articles.append(article)
                        
                except Exception as e:
                    logger.warning(f"è§£ææ–‡ç« å¤±è´¥: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"æå–æ–‡ç« å¤±è´¥: {e}")
            
        return articles
    
    def _parse_date(self, entry) -> datetime:
        """è§£ææ–‡ç« å‘å¸ƒæ—¥æœŸ"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                return datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                return datetime(*entry.updated_parsed[:6])
            else:
                return datetime.now()
        except:
            return datetime.now()
    
    def _is_recent_article(self, pub_date: datetime, hours=24) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ€è¿‘çš„æ–‡ç« """
        cutoff = datetime.now() - timedelta(hours=hours)
        return pub_date >= cutoff
    
    def filter_articles_by_keywords(self, articles: List[Article]) -> List[Article]:
        """æ ¹æ®å…³é”®è¯è¿‡æ»¤æ–‡ç« """
        keywords = self.config.get('KEYWORDS', [])
        if not keywords:
            return articles
            
        filtered_articles = []
        
        for article in articles:
            text_to_search = f"{article.title} {article.summary}".lower()
            matched_keywords = []
            
            for keyword in keywords:
                if keyword.lower() in text_to_search:
                    matched_keywords.append(keyword)
                    self.stats.record_keyword_match(keyword)
                    
            if matched_keywords:
                article.matched_keywords = matched_keywords
                filtered_articles.append(article)
                
        logger.info(f"å…³é”®è¯è¿‡æ»¤åå‰©ä½™ {len(filtered_articles)} ç¯‡æ–‡ç« ")
        return filtered_articles
    
    def calculate_article_scores(self, articles: List[Article]) -> List[Article]:
        """è®¡ç®—æ–‡ç« å¾—åˆ†"""
        zone_weights = self.config.get('ZONE_WEIGHTS', {1: 10.0, 2: 7.0, 3: 4.0, 4: 2.0})
        
        for article in articles:
            # åŸºç¡€åˆ†æ•°åŸºäºæœŸåˆŠåˆ†åŒº
            base_score = zone_weights.get(article.journal_zone, 1.0)
            
            # å…³é”®è¯åŒ¹é…å¥–åŠ±
            keyword_bonus = len(article.matched_keywords) * 2.0
            
            # æ—¶é—´è¡°å‡ï¼ˆè¶Šæ–°çš„æ–‡ç« åˆ†æ•°è¶Šé«˜ï¼‰
            time_factor = self._calculate_time_factor(article.published)
            
            article.score = base_score + keyword_bonus + time_factor
            
        # æŒ‰åˆ†æ•°æ’åº
        articles.sort(key=lambda x: x.score, reverse=True)
        return articles
    
    def _calculate_time_factor(self, pub_date: datetime) -> float:
        """è®¡ç®—æ—¶é—´å› å­"""
        hours_ago = (datetime.now() - pub_date).total_seconds() / 3600
        # 24å°æ—¶å†…çš„æ–‡ç« æ—¶é—´å¥–åŠ±é€’å‡
        if hours_ago <= 24:
            return 5.0 * (1 - hours_ago / 24)
        return 0.0
    
    def generate_trending_phrases(self, articles: List[Article]) -> List[Tuple[str, float]]:
        """ç”Ÿæˆçƒ­é—¨çŸ­è¯­"""
        if not self.config.get('DYNAMIC_PHRASE_EXTRACTION', False):
            return []
            
        texts = []
        for article in articles:
            texts.append(f"{article.title} {article.summary}")
            
        return self.phrase_extractor.extract_phrases(texts, top_k=20)
    
    def format_message(self, articles: List[Article], trending_phrases: List[Tuple[str, float]]) -> str:
        """æ ¼å¼åŒ–æ¨é€æ¶ˆæ¯"""
        today = datetime.now().strftime('%Y-%m-%d')
        
        message = f"ã€åœ°è´¨å‰æ²¿æ¯æ—¥æ¨é€ã€‘{today}\n\n"
        
        if articles:
            message += f"ğŸ”¬ ä»Šæ—¥ç²¾é€‰æ–‡ç«  ({len(articles)}ç¯‡):\n\n"
            
            for i, article in enumerate(articles[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ç¯‡
                title = article.title
                
                # ç¿»è¯‘æ ‡é¢˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.translator and self.config.get('TRANSLATE_TO_CHINESE', False):
                    try:
                        translated_title = self.translator.translate_text(title)
                        if translated_title != title:
                            title = f"{title}\n   è¯‘: {translated_title}"
                    except:
                        pass
                
                zone_emoji = {1: "â­â­â­", 2: "â­â­", 3: "â­", 4: ""}
                zone_indicator = zone_emoji.get(article.journal_zone, "")
                
                message += f"{i}. {title} {zone_indicator}\n"
                message += f"   {article.link}\n"
                message += f"   æ¥æº: {article.source} | åˆ†æ•°: {article.score:.1f}\n"
                
                if article.matched_keywords:
                    message += f"   åŒ¹é…å…³é”®è¯: {', '.join(article.matched_keywords)}\n"
                    
                message += "\n"
        else:
            message += "ä»Šæ—¥æš‚æ— åŒ¹é…çš„æ–‡ç« \n\n"
        
        # æ·»åŠ çƒ­é—¨çŸ­è¯­
        if trending_phrases:
            message += "ã€ä»Šæ—¥çƒ­ç‚¹çŸ­è¯­ã€‘\n"
            for phrase, score in trending_phrases[:10]:
                message += f"â€¢ {phrase} ({score:.3f})\n"
            message += "\n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if self.config.get('ENABLE_DETAILED_STATS', False):
            stats_summary = self.stats.get_summary()
            message += f"ã€ç»Ÿè®¡ä¿¡æ¯ã€‘\n"
            message += f"å¤„ç†æ–‡ç« : {stats_summary['daily_stats'].get('articles_processed', 0)}\n"
            message += f"RSSæºè®¿é—®: {sum(v for k, v in stats_summary['daily_stats'].items() if k.startswith('journal_'))}\n"
            
        return message
    
    def send_wechat_message(self, message: str) -> bool:
        """å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯"""
        webhook_url = self.config.get('WECHAT_WEBHOOK', '')
        if not webhook_url:
            logger.warning("æœªé…ç½®ä¼ä¸šå¾®ä¿¡Webhookåœ°å€")
            return False
            
        payload = {
            "msgtype": "text",
            "text": {
                "content": message
            }
        }
        
        try:
            response = requests.post(webhook_url, json=payload, timeout=10)
            response.raise_for_status()
            
            result = response.json()
            if result.get('errcode') == 0:
                logger.info("ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯å‘é€æˆåŠŸ")
                self.stats.record_event("wechat_success")
                return True
            else:
                logger.error(f"ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯å‘é€å¤±è´¥: {result}")
                self.stats.record_event("wechat_error")
                return False
                
        except Exception as e:
            logger.error(f"å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯å¼‚å¸¸: {e}")
            self.stats.record_event("wechat_error")
            return False
    
    def run_daily_scan(self):
        """æ‰§è¡Œæ¯æ—¥æ‰«æ"""
        logger.info("å¼€å§‹æ¯æ—¥RSSæ‰«æ")
        start_time = time.time()
        
        try:
            # è·å–RSSæº
            rss_sources = self.get_rss_sources()
            if not rss_sources:
                logger.warning("æ²¡æœ‰å¯ç”¨çš„RSSæº")
                return
            
            all_articles = []
            
            # è·å–æ‰€æœ‰æ–‡ç« 
            for url, zone in rss_sources:
                logger.info(f"æ­£åœ¨å¤„ç†RSSæº: {url}")
                
                feed = self.fetch_rss_feed(url)
                if feed:
                    articles = self.extract_articles(feed, (url, zone))
                    all_articles.extend(articles)
                    self.stats.record_event("rss_success")
                    
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            self.stats.record_event("articles_processed", len(all_articles))
            logger.info(f"å…±è·å–åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")
            
            # å…³é”®è¯è¿‡æ»¤
            filtered_articles = self.filter_articles_by_keywords(all_articles)
            
            # è®¡ç®—åˆ†æ•°å¹¶æ’åº
            scored_articles = self.calculate_article_scores(filtered_articles)
            
            # ç”Ÿæˆçƒ­é—¨çŸ­è¯­
            trending_phrases = self.generate_trending_phrases(scored_articles)
            
            # æ ¼å¼åŒ–æ¶ˆæ¯
            message = self.format_message(scored_articles, trending_phrases)
            
            # å‘é€æ¶ˆæ¯
            if scored_articles or self.config.get('SEND_EMPTY_REPORTS', False):
                self.send_wechat_message(message)
            
            # ä¿å­˜ç»Ÿè®¡æ•°æ®
            self.stats.save_daily_stats()
            
            elapsed_time = time.time() - start_time
            logger.info(f"æ¯æ—¥æ‰«æå®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’")
            
        except Exception as e:
            logger.error(f"æ¯æ—¥æ‰«æå¤±è´¥: {e}")
            self.stats.record_event("scan_error")
            raise

def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_file = CONFIG_DIR / "config.py"
        if not config_file.exists():
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            logger.info("è¯·å¤åˆ¶ config.py.example ä¸º config.py å¹¶å¡«å…¥æ­£ç¡®çš„é…ç½®")
            return
        
        # åˆ›å»ºå¹¶è¿è¡Œç›‘æ§å™¨
        sniffer = EnhancedGeoSniffer()
        sniffer.run_daily_scan()
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œå¼‚å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()