#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åœ°çƒç§‘å­¦RSSç›‘æ§ç³»ç»Ÿ - å¸¦åŒºåŸŸè¯„åˆ†çš„æ¯æ—¥ç›‘æ§å™¨
GeoScience RSS Sniffer with Zone Scoring - Daily Monitor

åŠŸèƒ½æ”¹è¿›ç‰ˆæœ¬ï¼š
1. å•æ¬¡æ¨é€æœ€å¤š6æ¡å†…å®¹ï¼ˆä»åŸæ¥çš„3æ¡å¢åŠ åˆ°6æ¡ï¼‰
2. ä¸æ˜¾ç¤ºæ–‡ç« æ‘˜è¦
3. å½“æ–‡ç« è¶…è¿‡6æ¡æ—¶ï¼Œåˆ†ä¸¤æ¬¡æ¨é€ï¼Œç¬¬äºŒæ¬¡æ¨é€ä¸è¶…è¿‡6æ¡
4. å½“å½“æ—¥æ–°æ–‡ç« ä¸è¶³6æ¡æ—¶ï¼Œè‡ªåŠ¨ä»ä¹‹å‰æœªæ¨é€çš„é˜Ÿåˆ—ä¸­è¡¥å……æ–‡ç« 

Author: F-swanlight
Date: 2025-01-20
"""

import os
import sys
import json
import time
import logging
import requests
import feedparser
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import pickle

# æ·»åŠ é…ç½®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'config'))

# æ ¸å¿ƒå¸¸é‡é…ç½®
MAX_DAILY_PUSH = 6  # ä¿®æ”¹ï¼šä»3æ”¹ä¸º6ï¼Œå•æ¬¡æœ€å¤šæ¨é€6æ¡å†…å®¹
MAX_SECOND_PUSH = 6  # ç¬¬äºŒæ¬¡æ¨é€æœ€å¤š6æ¡
MIN_ARTICLES_FOR_PUSH = 1  # æœ€å°‘æ¨é€æ–‡ç« æ•°é‡

# æ•°æ®æŒä¹…åŒ–æ–‡ä»¶
HISTORICAL_QUEUE_FILE = os.path.join(os.path.dirname(__file__), '..', 'data', 'historical_queue.pkl')
PUSHED_ARTICLES_FILE = os.path.join(os.path.dirname(__file__), '..', 'data', 'pushed_articles.pkl')

@dataclass
class Article:
    """æ–‡ç« æ•°æ®ç»“æ„"""
    title: str
    link: str
    summary: str
    published: str
    source: str
    keywords_matched: List[str]
    score: float = 0.0
    zone_score: float = 0.0
    is_pushed: bool = False
    push_date: Optional[str] = None

class RSSMonitor:
    """RSSç›‘æ§å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.setup_logging()
        self.load_config()
        self.historical_queue: List[Article] = []
        self.pushed_articles: List[str] = []  # å­˜å‚¨å·²æ¨é€æ–‡ç« çš„é“¾æ¥
        self.load_historical_data()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = os.path.join(os.path.dirname(__file__), '..', 'logs')
        os.makedirs(log_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(log_dir, 'sniffer.log'), encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def load_config(self):
        """åŠ è½½é…ç½®"""
        try:
            import config
            self.wechat_webhook = getattr(config, 'WECHAT_WEBHOOK', '')
            self.rss_feeds = getattr(config, 'RSS_FEEDS', self.get_default_feeds())
            self.keywords = getattr(config, 'KEYWORDS', self.get_default_keywords())
        except ImportError:
            self.logger.warning("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.wechat_webhook = ''
            self.rss_feeds = self.get_default_feeds()
            self.keywords = self.get_default_keywords()
            
    def get_default_feeds(self) -> List[str]:
        """è·å–é»˜è®¤RSSæº"""
        return [
            "https://www.nature.com/ngeo.rss",
            "https://www.sciencemag.org/rss/current.xml",
            "https://pubs.geoscienceworld.org/rss/geology/current.xml",
            "https://www.frontiersin.org/research-topics/rss",
        ]
        
    def get_default_keywords(self) -> List[str]:
        """è·å–é»˜è®¤å…³é”®è¯"""
        return [
            "carbonate", "ç¢³é…¸ç›å²©", "limestone", "dolomite",
            "microbialite", "å¾®ç”Ÿç‰©çŸ¿åŒ–", "stromatolite", "thrombolite",
            "natural hydrogen", "å¤©ç„¶æ°¢", "hydrogen seepage",
            "geochemistry", "åœ°çƒåŒ–å­¦", "isotope", "åŒä½ç´ ",
            "sedimentology", "æ²‰ç§¯å­¦", "paleontology", "å¤ç”Ÿç‰©å­¦",
        ]
        
    def load_historical_data(self):
        """åŠ è½½å†å²æ•°æ®"""
        data_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
        os.makedirs(data_dir, exist_ok=True)
        
        # åŠ è½½å†å²é˜Ÿåˆ—
        if os.path.exists(HISTORICAL_QUEUE_FILE):
            try:
                with open(HISTORICAL_QUEUE_FILE, 'rb') as f:
                    queue_data = pickle.load(f)
                    self.historical_queue = [Article(**item) if isinstance(item, dict) else item for item in queue_data]
                self.logger.info(f"åŠ è½½å†å²é˜Ÿåˆ—: {len(self.historical_queue)} ç¯‡æ–‡ç« ")
            except Exception as e:
                self.logger.error(f"åŠ è½½å†å²é˜Ÿåˆ—å¤±è´¥: {e}")
                self.historical_queue = []
                
        # åŠ è½½å·²æ¨é€æ–‡ç« åˆ—è¡¨
        if os.path.exists(PUSHED_ARTICLES_FILE):
            try:
                with open(PUSHED_ARTICLES_FILE, 'rb') as f:
                    self.pushed_articles = pickle.load(f)
                self.logger.info(f"åŠ è½½å·²æ¨é€æ–‡ç« è®°å½•: {len(self.pushed_articles)} ç¯‡")
            except Exception as e:
                self.logger.error(f"åŠ è½½å·²æ¨é€æ–‡ç« è®°å½•å¤±è´¥: {e}")
                self.pushed_articles = []
                
    def save_historical_data(self):
        """ä¿å­˜å†å²æ•°æ®"""
        try:
            # ä¿å­˜å†å²é˜Ÿåˆ—
            with open(HISTORICAL_QUEUE_FILE, 'wb') as f:
                queue_data = [asdict(article) for article in self.historical_queue]
                pickle.dump(queue_data, f)
                
            # ä¿å­˜å·²æ¨é€æ–‡ç« åˆ—è¡¨
            with open(PUSHED_ARTICLES_FILE, 'wb') as f:
                pickle.dump(self.pushed_articles, f)
                
            self.logger.info("å†å²æ•°æ®ä¿å­˜æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"ä¿å­˜å†å²æ•°æ®å¤±è´¥: {e}")
            
    def fetch_rss_articles(self) -> List[Article]:
        """è·å–RSSæ–‡ç« """
        all_articles = []
        today = datetime.now().date()
        
        for feed_url in self.rss_feeds:
            try:
                self.logger.info(f"æ­£åœ¨è·å–RSSæº: {feed_url}")
                
                # è®¾ç½®è¯·æ±‚å¤´
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                response = requests.get(feed_url, headers=headers, timeout=30)
                response.raise_for_status()
                
                feed = feedparser.parse(response.content)
                source_name = feed.feed.get('title', feed_url)
                
                for entry in feed.entries:
                    # æ£€æŸ¥å‘å¸ƒæ—¶é—´ï¼ˆåªå¤„ç†ä»Šæ—¥æ–‡ç« ï¼‰
                    pub_date = self.parse_publish_date(entry)
                    if pub_date and pub_date.date() != today:
                        continue
                        
                    # æ£€æŸ¥å…³é”®è¯åŒ¹é…
                    matched_keywords = self.check_keywords(entry)
                    if not matched_keywords:
                        continue
                        
                    # è·³è¿‡å·²æ¨é€çš„æ–‡ç« 
                    if entry.link in self.pushed_articles:
                        continue
                        
                    article = Article(
                        title=entry.title,
                        link=entry.link,
                        summary=entry.get('summary', ''),
                        published=pub_date.isoformat() if pub_date else '',
                        source=source_name,
                        keywords_matched=matched_keywords,
                        score=len(matched_keywords),
                        zone_score=self.calculate_zone_score(entry, matched_keywords)
                    )
                    
                    all_articles.append(article)
                    
            except Exception as e:
                self.logger.error(f"è·å–RSSæºå¤±è´¥ {feed_url}: {e}")
                continue
                
        self.logger.info(f"ä»Šæ—¥å…±è·å–åˆ° {len(all_articles)} ç¯‡ç¬¦åˆæ¡ä»¶çš„æ–°æ–‡ç« ")
        return all_articles
        
    def parse_publish_date(self, entry) -> Optional[datetime]:
        """è§£æå‘å¸ƒæ—¶é—´"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                return datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                return datetime(*entry.updated_parsed[:6])
            elif hasattr(entry, 'published'):
                from dateutil.parser import parse
                return parse(entry.published)
        except Exception:
            pass
        return None
        
    def check_keywords(self, entry) -> List[str]:
        """æ£€æŸ¥å…³é”®è¯åŒ¹é…"""
        content = f"{entry.title} {entry.get('summary', '')}".lower()
        matched = []
        
        for keyword in self.keywords:
            if keyword.lower() in content:
                matched.append(keyword)
                
        return matched
        
    def calculate_zone_score(self, entry, matched_keywords: List[str]) -> float:
        """è®¡ç®—åŒºåŸŸè¯„åˆ†"""
        base_score = len(matched_keywords)
        
        # æ ¹æ®æ¥æºåŠ æƒ
        source_weights = {
            'nature': 3.0,
            'science': 3.0,
            'geology': 2.5,
            'earth': 2.0,
            'geochemistry': 2.0,
        }
        
        source = entry.get('source', '').lower()
        weight = 1.0
        for key, value in source_weights.items():
            if key in source:
                weight = value
                break
                
        return base_score * weight
        
    def format_article_for_push(self, article: Article) -> str:
        """æ ¼å¼åŒ–æ–‡ç« ç”¨äºæ¨é€ï¼ˆç§»é™¤æ‘˜è¦éƒ¨åˆ†ï¼‰"""
        # ä¿®æ”¹ï¼šç§»é™¤æ‘˜è¦éƒ¨åˆ†ï¼Œåªä¿ç•™æ ‡é¢˜å’Œé“¾æ¥
        return f"{article.title}\n{article.link}"
        
    def get_backfill_articles(self, needed_count: int) -> List[Article]:
        """ä»å†å²é˜Ÿåˆ—ä¸­è·å–è¡¥å……æ–‡ç« """
        if needed_count <= 0:
            return []
            
        # è¿‡æ»¤æœªæ¨é€çš„æ–‡ç« ï¼ŒæŒ‰è¯„åˆ†æ’åº
        available_articles = [
            article for article in self.historical_queue
            if not article.is_pushed and article.link not in self.pushed_articles
        ]
        
        # æŒ‰æ€»è¯„åˆ†é™åºæ’åº
        available_articles.sort(key=lambda x: x.zone_score + x.score, reverse=True)
        
        # å–æ‰€éœ€æ•°é‡
        backfill_articles = available_articles[:needed_count]
        
        self.logger.info(f"ä»å†å²é˜Ÿåˆ—ä¸­è¡¥å……äº† {len(backfill_articles)} ç¯‡æ–‡ç« ")
        return backfill_articles
        
    def process_daily_articles(self) -> Tuple[List[List[Article]], Dict[str, int]]:
        """å¤„ç†æ¯æ—¥æ–‡ç« ï¼Œè¿”å›æ¨é€æ‰¹æ¬¡å’Œå…³é”®è¯ç»Ÿè®¡"""
        # è·å–ä»Šæ—¥æ–°æ–‡ç« 
        today_articles = self.fetch_rss_articles()
        
        # æ›´æ–°å†å²é˜Ÿåˆ—
        self.historical_queue.extend(today_articles)
        
        # æŒ‰è¯„åˆ†æ’åºä»Šæ—¥æ–‡ç« 
        today_articles.sort(key=lambda x: x.zone_score + x.score, reverse=True)
        
        push_batches = []
        
        if len(today_articles) <= MAX_DAILY_PUSH:
            # å½“æ—¥æ–‡ç« ä¸è¶³MAX_DAILY_PUSHæ¡ï¼Œéœ€è¦è¡¥å……
            needed_count = MAX_DAILY_PUSH - len(today_articles)
            backfill_articles = self.get_backfill_articles(needed_count)
            
            # åˆå¹¶æ–‡ç« åˆ—è¡¨
            combined_articles = today_articles + backfill_articles
            
            if combined_articles:
                push_batches.append(combined_articles)
                
        else:
            # å½“æ—¥æ–‡ç« è¶…è¿‡MAX_DAILY_PUSHæ¡ï¼Œéœ€è¦åˆ†æ‰¹æ¨é€
            # ç¬¬ä¸€æ‰¹ï¼šå‰MAX_DAILY_PUSHæ¡
            first_batch = today_articles[:MAX_DAILY_PUSH]
            push_batches.append(first_batch)
            
            # ç¬¬äºŒæ‰¹ï¼šå‰©ä½™æ–‡ç« ï¼Œä½†ä¸è¶…è¿‡MAX_SECOND_PUSHæ¡
            remaining_articles = today_articles[MAX_DAILY_PUSH:]
            if remaining_articles:
                second_batch = remaining_articles[:MAX_SECOND_PUSH]
                push_batches.append(second_batch)
                
        # ç»Ÿè®¡å…³é”®è¯
        keyword_stats = self.calculate_keyword_stats(today_articles)
        
        return push_batches, keyword_stats
        
    def calculate_keyword_stats(self, articles: List[Article]) -> Dict[str, int]:
        """è®¡ç®—å…³é”®è¯ç»Ÿè®¡"""
        stats = {}
        for article in articles:
            for keyword in article.keywords_matched:
                stats[keyword] = stats.get(keyword, 0) + 1
        return stats
        
    def send_wechat_message(self, content: str) -> bool:
        """å‘é€å¾®ä¿¡æ¶ˆæ¯"""
        if not self.wechat_webhook:
            self.logger.warning("æœªé…ç½®å¾®ä¿¡Webhookï¼Œè·³è¿‡æ¨é€")
            return False
            
        try:
            payload = {
                "msgtype": "text",
                "text": {
                    "content": content
                }
            }
            
            response = requests.post(
                self.wechat_webhook,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get('errcode') == 0:
                self.logger.info("å¾®ä¿¡æ¨é€æˆåŠŸ")
                return True
            else:
                self.logger.error(f"å¾®ä¿¡æ¨é€å¤±è´¥: {result}")
                return False
                
        except Exception as e:
            self.logger.error(f"å¾®ä¿¡æ¨é€å¼‚å¸¸: {e}")
            return False
            
    def create_push_message(self, articles: List[Article], batch_num: int, total_batches: int, keyword_stats: Dict[str, int] = None) -> str:
        """åˆ›å»ºæ¨é€æ¶ˆæ¯"""
        today = datetime.now().strftime('%Y-%m-%d')
        
        # æ¶ˆæ¯å¤´éƒ¨
        if total_batches > 1:
            header = f"ã€åœ°è´¨å‰æ²¿æ¯æ—¥æ¨é€ã€‘{today} (ç¬¬{batch_num}/{total_batches}æ‰¹)"
        else:
            header = f"ã€åœ°è´¨å‰æ²¿æ¯æ—¥æ¨é€ã€‘{today}"
            
        content_lines = [header, ""]
        content_lines.append(f"ğŸ”¬ ä»Šæ—¥ç²¾é€‰æ–‡ç«  ({len(articles)}ç¯‡):")
        content_lines.append("")
        
        # æ–‡ç« åˆ—è¡¨
        for i, article in enumerate(articles, 1):
            article_text = self.format_article_for_push(article)
            content_lines.append(f"{i}. {article_text}")
            content_lines.append("")
            
        # åªåœ¨æœ€åä¸€æ‰¹æ·»åŠ å…³é”®è¯ç»Ÿè®¡
        if batch_num == total_batches and keyword_stats:
            content_lines.append("ã€ä»Šæ—¥çƒ­ç‚¹è¯æ±‡æ’è¡Œã€‘")
            sorted_keywords = sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True)
            for keyword, count in sorted_keywords[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                content_lines.append(f"{keyword}: {count}")
                
        return "\n".join(content_lines)
        
    def mark_articles_as_pushed(self, articles: List[Article]):
        """æ ‡è®°æ–‡ç« ä¸ºå·²æ¨é€"""
        today = datetime.now().strftime('%Y-%m-%d')
        
        for article in articles:
            article.is_pushed = True
            article.push_date = today
            
            # æ·»åŠ åˆ°å·²æ¨é€åˆ—è¡¨
            if article.link not in self.pushed_articles:
                self.pushed_articles.append(article.link)
                
        # æ¸…ç†å†å²é˜Ÿåˆ—ä¸­çš„å·²æ¨é€æ–‡ç« ï¼ˆä¿ç•™æœ€è¿‘30å¤©çš„è®°å½•ï¼‰
        self.cleanup_historical_queue()
        
    def cleanup_historical_queue(self):
        """æ¸…ç†å†å²é˜Ÿåˆ—"""
        cutoff_date = datetime.now() - timedelta(days=30)
        
        # ä¿ç•™æœªæ¨é€çš„æ–‡ç« å’Œæœ€è¿‘30å¤©çš„å·²æ¨é€æ–‡ç« 
        cleaned_queue = []
        for article in self.historical_queue:
            if not article.is_pushed:
                cleaned_queue.append(article)
            elif article.push_date:
                try:
                    push_date = datetime.fromisoformat(article.push_date)
                    if push_date >= cutoff_date:
                        cleaned_queue.append(article)
                except:
                    pass
                    
        self.historical_queue = cleaned_queue
        
        # æ¸…ç†å·²æ¨é€æ–‡ç« åˆ—è¡¨ï¼ˆä¿ç•™æœ€è¿‘1000æ¡è®°å½•ï¼‰
        if len(self.pushed_articles) > 1000:
            self.pushed_articles = self.pushed_articles[-1000:]
            
    def run_daily_check(self):
        """æ‰§è¡Œæ¯æ—¥æ£€æŸ¥"""
        self.logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥RSSç›‘æ§æ£€æŸ¥")
        
        try:
            # å¤„ç†æ¯æ—¥æ–‡ç« 
            push_batches, keyword_stats = self.process_daily_articles()
            
            if not push_batches:
                self.logger.info("ä»Šæ—¥æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ç« éœ€è¦æ¨é€")
                return
                
            # åˆ†æ‰¹æ¨é€
            total_batches = len(push_batches)
            for batch_num, articles in enumerate(push_batches, 1):
                # åˆ›å»ºæ¨é€æ¶ˆæ¯
                message = self.create_push_message(
                    articles, 
                    batch_num, 
                    total_batches, 
                    keyword_stats if batch_num == total_batches else None
                )
                
                # å‘é€æ¨é€
                success = self.send_wechat_message(message)
                
                if success:
                    # æ ‡è®°ä¸ºå·²æ¨é€
                    self.mark_articles_as_pushed(articles)
                    self.logger.info(f"ç¬¬{batch_num}æ‰¹æ¨é€æˆåŠŸï¼ŒåŒ…å«{len(articles)}ç¯‡æ–‡ç« ")
                else:
                    self.logger.error(f"ç¬¬{batch_num}æ‰¹æ¨é€å¤±è´¥")
                    
                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                if batch_num < total_batches:
                    time.sleep(5)
                    
            # ä¿å­˜æ•°æ®
            self.save_historical_data()
            
            self.logger.info(f"æ¯æ—¥æ£€æŸ¥å®Œæˆï¼Œå…±æ¨é€{sum(len(batch) for batch in push_batches)}ç¯‡æ–‡ç« ")
            
        except Exception as e:
            self.logger.error(f"æ¯æ—¥æ£€æŸ¥æ‰§è¡Œå¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    try:
        monitor = RSSMonitor()
        monitor.run_daily_check()
    except Exception as e:
        logging.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()